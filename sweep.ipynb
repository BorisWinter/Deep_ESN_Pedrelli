{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "infrared-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from DeepESN import DeepESN\n",
    "from utils import MSE, load_chest, select_indexes\n",
    "import copy\n",
    "class Struct(object): pass\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "approximate-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Nr, Nl, reg, transient, rhos, lis, iss, con):\n",
    "    configs = Struct()\n",
    " \n",
    "    configs.rhos = rhos\n",
    "    configs.lis = lis\n",
    "    configs.iss = iss\n",
    "\n",
    "    configs.IPconf = Struct()\n",
    "    configs.IPconf.DeepIP = 0 # deactivate pre-train\n",
    "\n",
    "    configs.reservoirConf = Struct()\n",
    "    configs.reservoirConf.connectivity = con \n",
    "\n",
    "    configs.readout = Struct()\n",
    "    configs.readout.trainMethod = 'SVD' # train with singular value decomposition (more accurate)\n",
    "    configs.readout.regularizations = 10.0**np.array(range(-16,-1,1))\n",
    "\n",
    "    deepESN = DeepESN(Nu, Nr, Nl, configs)\n",
    "    states = deepESN.computeState(dataset.inputs, deepESN.IPconf.DeepIP)\n",
    "        \n",
    "    train_states = select_indexes(states, list(TR_indexes) + list(VL_indexes), transient)\n",
    "    train_targets = select_indexes(dataset.targets, list(TR_indexes) + list(VL_indexes), transient)\n",
    "    test_states = select_indexes(states, TS_indexes)\n",
    "    test_targets = select_indexes(dataset.targets, TS_indexes)\n",
    "    \n",
    "    deepESN.trainReadout(train_states, train_targets, reg)\n",
    "\n",
    "    return score(deepESN, train_states, test_states, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "operational-accommodation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(deepESN, train_states, test_states, test_targets):\n",
    "    #compute error in standard way\n",
    "    test_states_std = copy.deepcopy(test_states)\n",
    "    test_outputs_std = deepESN.computeOutput(test_states_std)\n",
    "    test_error_std = MSE(test_outputs_std, test_targets)\n",
    "\n",
    "    #compute error in our way\n",
    "    test_outputs = []\n",
    "    state = [train_states[0][:,-1:]] #set last training state as initial state\n",
    "    output = [np.array(deepESN.computeOutput(state))] #set last training output initial output\n",
    "    for t in range(len(test_states[0][0])):\n",
    "        state = deepESN.computeState(inputs=output, DeepIP=0, initialStates=state[0])\n",
    "        output = deepESN.computeOutput(state)\n",
    "        test_outputs.append(output)\n",
    "        output = [np.array(output)]\n",
    "#             if t % 3000 == 0:\n",
    "#                 print(t)\n",
    "#                 print(output)\n",
    "\n",
    "    test_error = MSE(np.array(test_outputs), np.array(test_targets))\n",
    "    return test_error, test_outputs, test_error_std, test_outputs_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "signed-grant",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = train(10, 2, 0.00, 100, 0.9, 1.0, 0.1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "together-letter",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data length: 10000  Validating data length: 10000\nnumber of models to be swept over: 5832\n{'transient': 1000, 'rhos': 0.2, 'reg': 0.0, 'lis': 0.1, 'iss': 0.1, 'con': 0.2, 'Nr': 10, 'Nl': 1, '.sampling': 1}\n{'transient': 1000, 'rhos': 0.5, 'reg': 0.0, 'lis': 0.1, 'iss': 0.1, 'con': 0.2, 'Nr': 10, 'Nl': 1, '.sampling': 1}\n{'transient': 1000, 'rhos': 1.0, 'reg': 0.0, 'lis': 0.1, 'iss': 0.1, 'con': 0.2, 'Nr': 10, 'Nl': 1, '.sampling': 1}\n{'transient': 1000, 'rhos': 0.2, 'reg': 0.1, 'lis': 0.1, 'iss': 0.1, 'con': 0.2, 'Nr': 10, 'Nl': 1, '.sampling': 1}\n{'transient': 1000, 'rhos': 0.5, 'reg': 0.1, 'lis': 0.1, 'iss': 0.1, 'con': 0.2, 'Nr': 10, 'Nl': 1, '.sampling': 1}\n{'transient': 1000, 'rhos': 1.0, 'reg': 0.1, 'lis': 0.1, 'iss': 0.1, 'con': 0.2, 'Nr': 10, 'Nl': 1, '.sampling': 1}\n{'transient': 1000, 'rhos': 0.2, 'reg': 1.0, 'lis': 0.1, 'iss': 0.1, 'con': 0.2, 'Nr': 10, 'Nl': 1, '.sampling': 1}\n{'transient': 1000, 'rhos': 0.5, 'reg': 1.0, 'lis': 0.1, 'iss': 0.1, 'con': 0.2, 'Nr': 10, 'Nl': 1, '.sampling': 1}\n{'transient': 1000, 'rhos': 1.0, 'reg': 1.0, 'lis': 0.1, 'iss': 0.1, 'con': 0.2, 'Nr': 10, 'Nl': 1, '.sampling': 1}\n{'transient': 1000, 'rhos': 0.2, 'reg': 0.0, 'lis': 0.2, 'iss': 0.1, 'con': 0.2, 'Nr': 10, 'Nl': 1, '.sampling': 1}\n"
     ]
    }
   ],
   "source": [
    "#construct parameter grid\n",
    "\n",
    "#set parameter ranges for sweep\n",
    "parameters = {\n",
    "    \"Nr\": [10, 100, 200],          # number of recurrent units\n",
    "    \"Nl\": [1, 2, 5, 10],                     # number of recurrent layers\n",
    "    \"reg\": [0.0, 0.1, 1.0],                  # regularization of svd\n",
    "    \"transient\": [1000],                 # washout\n",
    "    \"rhos\": [0.2, 0.5, 1.0],       # set spectral radius for all recurrent layers\n",
    "    \"lis\": [0.1, 0.2, 0.5],            # set leaky rate for all recurrent layers\n",
    "    \"iss\": [0.1, 1.0],               # set input scale for all recurrent layers\n",
    "    \"con\": [0.2, 0.5, 1.0],                 # connectivity of recurrent matrix\n",
    "    \".sampling\": [1,10,25]             # set step size for sampling, 1 is no sampling. be careful not to make dataset too small\n",
    "}\n",
    "\n",
    "data_size = 20000 # pick dataset length to run parameter sweep on\n",
    "\n",
    "if 640640/max(parameters[\".sampling\"]) < data_size:\n",
    "    print(\"WARNING: highest sampling rate means that resulting dataset is smaller than data_size. \\n\")\n",
    "\n",
    "print(\"Training data length: \" + str(int(data_size*0.5)) + \"  Validating data length: \" + str(int(data_size*0.5)))\n",
    "\n",
    "search_space = ParameterGrid(parameters)\n",
    "print(\"number of models to be swept over: \" + str(len(search_space)))\n",
    "for i in range(10):\n",
    "    print(search_space[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "dress-economy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'transient': 1000,\n",
       "  'rhos': 1.0,\n",
       "  'reg': 0.1,\n",
       "  'lis': 0.1,\n",
       "  'iss': 1.0,\n",
       "  'con': 0.2,\n",
       "  'Nr': 200,\n",
       "  'Nl': 10,\n",
       "  '.sampling': 1},\n",
       " {'transient': 1000,\n",
       "  'rhos': 1.0,\n",
       "  'reg': 0.0,\n",
       "  'lis': 0.5,\n",
       "  'iss': 1.0,\n",
       "  'con': 1.0,\n",
       "  'Nr': 200,\n",
       "  'Nl': 5,\n",
       "  '.sampling': 1},\n",
       " {'transient': 1000,\n",
       "  'rhos': 0.2,\n",
       "  'reg': 1.0,\n",
       "  'lis': 0.2,\n",
       "  'iss': 1.0,\n",
       "  'con': 0.5,\n",
       "  'Nr': 200,\n",
       "  'Nl': 1,\n",
       "  '.sampling': 10},\n",
       " {'transient': 1000,\n",
       "  'rhos': 0.2,\n",
       "  'reg': 0.0,\n",
       "  'lis': 0.2,\n",
       "  'iss': 0.1,\n",
       "  'con': 0.5,\n",
       "  'Nr': 100,\n",
       "  'Nl': 1,\n",
       "  '.sampling': 25}]"
      ]
     },
     "metadata": {},
     "execution_count": 292
    }
   ],
   "source": [
    "search_space_short = []\n",
    "for i in range(25):\n",
    "    index = round(random.random() * len(search_space))\n",
    "    search_space_short.append(search_space[index])\n",
    "    \n",
    "search_space_short = sorted(search_space_short, key=lambda k: k['.sampling'])\n",
    "search_space_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "beneficial-annotation",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input length =  19999\n",
      ".7 =  13999\n",
      ".8 =  15999\n",
      "-1 =  19998\n",
      "Done loading data\n",
      "0\n",
      "1\n",
      "input length =  19999\n",
      ".7 =  13999\n",
      ".8 =  15999\n",
      "-1 =  19998\n",
      "Done loading data\n",
      "2\n",
      "input length =  19999\n",
      ".7 =  13999\n",
      ".8 =  15999\n",
      "-1 =  19998\n",
      "Done loading data\n",
      "3\n",
      "average time taken: 17.847159564495087\n"
     ]
    }
   ],
   "source": [
    "# run the parameter sweep\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "results = []\n",
    "previous = None\n",
    "\n",
    "for i, args in enumerate(search_space_short):\n",
    "\n",
    "    if args[\".sampling\"] != previous:\n",
    "        #train-test data split 50/50\n",
    "        dataset, Nu, TR_indexes, VL_indexes, TS_indexes = load_chest('datasets', data_size, args[\".sampling\"])\n",
    "        previous = args[\".sampling\"]\n",
    "    results.append(train(Nr=args['Nr'], Nl=args['Nl'], reg=args['reg'], transient=args['transient'], rhos=args['rhos'], lis=args['lis'], iss=args['iss'], con=args['con']))\n",
    "\n",
    "    print(i)\n",
    "\n",
    "print(\"average time taken: \" + str((time.time()-start)/len(search_space_short)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = []\n",
    "for key in search_space_short[0].keys():\n",
    "    keys.append(key)\n",
    "keys.append(\"result[0]\")\n",
    "keys.append(\"result[1]\")\n",
    "keys.append(\"result[2]\")\n",
    "keys.append(\"result[3]\")\n",
    "\n",
    "header = keys\n",
    "\n",
    "with open('results.csv', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(header)\n",
    "\n",
    "    for i in range(len(search_space_short)):\n",
    "        vals = []\n",
    "        for val in search_space_short[i].values():\n",
    "            vals.append(val)\n",
    "        for j in range(len(results[0])):\n",
    "            vals.append(results[j])\n",
    "        write.writerow(vals)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "e0bc0a773709f5870974e9780ab6d44df802de2accd45ec44390b3551e78f4ca"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}